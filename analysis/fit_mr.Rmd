---
title: "Multiple Regression Model"
author: "Donghyung Lee"
date: "`r Sys.Date()`"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multiple Regression Fit (Lecture 13)

### 1. Dataset 
The Prestige data frame is the Prestige of Canadian Occupations. It has 102 rows and 6 columns. The observations are occupations. First of all, we remove all observations that have missing values for varaible ``type''.
```{r}
#install.packages("car")
library(car)
#?Prestige
mydata=Prestige[!is.na(Prestige$type), ]#remove the observations with missing values for "type"
unique(mydata$type)
```

To have an idea about the predictor variable and the response, we can view the **enhanced scatterplot**.

```{r}
scatterplot(income~education|type, data=mydata,ylab="Income",
            xlab="Education",main="Income vs Education")
```
Based on the plot, maybe there's linear relationsihp between education and income. It seems the intercept of bc and prof are similar, different from that of the wc. We also want know whether the slope of the three groups are different (whether interaction term is necessary).

Alternative method using the ggplot:
```{r}
library(ggplot2)
ggplot(data=mydata, aes(x=education, y=income)) + 
  geom_point(aes(colour = factor(type))) +
  geom_smooth(aes(colour = factor(type)))
```



### 2. Generate the design matrix
```{r}
X=model.matrix(~type,data=mydata)
#X
Y=as.matrix(mydata$income)
solve(t(X)%*%X)%*%t(X)%*%Y
lm(income~type,data=mydata)#automatically, bc is the reference group.
```


### 3. Mix continuous and categorical variable.
#### a. Use income as response, education and type as predictor.
```{r}
X=model.matrix(~education+type,data=mydata)
#X
Y=as.matrix(mydata$income)
solve(t(X)%*%X)%*%t(X)%*%Y
lm(income~education+type,data=mydata)
#If the data is in letters, R recognize it as a categorical factor.
```

#### b. If you don't like bc be reference group, change it to wc.
```{r}
new=relevel(mydata$type,ref="wc")
X=model.matrix(~education+new,data=mydata)
Y=as.matrix(mydata$income)
solve(t(X)%*%X)%*%t(X)%*%Y
lm(income~education+new,data=mydata)
```

#### c. Like constructing a linear regression model, you may also include interaction terms
```{r}
X=model.matrix(~education+type+education*type,data=mydata)
Y=as.matrix(mydata$income)
solve(t(X)%*%X)%*%t(X)%*%Y
lm(income~education+type+education*type,data=mydata)
```

### 4. What if you want a categorical variable, but it's coded in 1,2,3, etc. 
#### a. Directly use it, R will recognize it as numeric variable.
```{r}
mydata2=cbind(mydata,c(rep(1,40),rep(2,38),rep(3,20)))
colnames(mydata2)[7]=c("group")
#mydata2
lm(income~group,data=mydata2)
```

#### b. We can manually let R know it's a categorical variable.
```{r}
mydata2$group=factor(mydata2$group)
lm(income~group,data=mydata2)

#alternative way
groupf=factor(mydata2$group)
lm(income~groupf,data=mydata2)

#alternative way
lm(income~factor(group),data=mydata2)
```


### 5. Different ways to code interaction term
Note: A:B in model output just means the interaciton of A and B.

#### a. 2 predictor variables, main effect+2way interaction
```{r}
library(car)
#?Prestige#If you don't have the car package, download it from canvas. 
#Or search for "Prestige data R download",run the code on the first part.
mydata=Prestige[!is.na(Prestige$type), ]#remove the observations with missing values for "type"

fit1=lm(income~education+type+education*type,data=mydata)
fit2=lm(income~education*type,data=mydata)#easier way to fit main effect+two way interactions.

summary(fit1)
summary(fit2)
anova(fit1)
anova(fit2)
```

#### b. 3 variables, main effect+2-way interaction+3way interaction
```{r}
fit3=lm(income~education*prestige*type,data=mydata)
summary(fit3)
anova(fit3)
```

#### c. More variables, main effect model
```{r}
colnames(mydata)
fit4=lm(income~education+women+prestige+census+type,data=mydata)
fit5=lm(income~., data=mydata)
summary(fit4)
summary(fit5)

anova(fit4)
anova(fit5)
```

#### d. More variables, main effect+2way interactions. use a subset, just to read it clearly.
```{r}
test=mydata[,c(1,2,3,6)]
colnames(test)
fit6=lm(income~education+women+type+education*women+education*type+women*type,data=test)
fit7=lm(income~.*.,data=test)
summary(fit6)
summary(fit7)

anova(fit6)
anova(fit7)

formula(fit7)#use this funciton to know what kind fo model you're fitting.
```

### 6. Compute the F statistic based on the ANOVA table.
Use the result of fit7 as an example.
```{r}
names(anova(fit7))
SSvec=anova(fit7)$"Sum Sq"
SSR=sum(SSvec[1:6])
#SSR=sum(SSvec[-7])
SSE=SSvec[7]
dfvec=anova(fit7)$Df
dfR=sum(dfvec[1:6])
#dfR=sum(dfvec[-7])
dfE=dfvec[7]
Fstat=(SSR/dfR)/(SSE/dfE)
Fstat
dfR
dfE
pval=1-pf(Fstat,dfR,dfE)
pval

#If alpha=0.05, the critical value can also be computed.
alpha=0.05
qf((1-alpha),dfR,dfE)
```


### 7. Comment about the commen steps to work on multivariate data. Use two predictor variables as an example.
##### (1) Do explorative data analysis. For example draw scatter plot, compute correlation matrix
##### (2) Most of the time, start with fitting a full interaction model.
##### (3) Test the interaction terms first. 
* If the the interaction term(s) is/are insignificant, delete them and fit an "equal slopes‚Äù model.
* If the interaction term(s) is are significant, the model cannot be further simplified, even though the main effects are not significant.

##### (4) If there is/are insignificant predictor(s) in an equal slope model, we may drop them.
```{r}
fit=lm(income~.*.,data=mydata[1:49,c(2,4,5)])
summary(fit)
anova(fit)
```



