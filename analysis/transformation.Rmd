---
title: "Transformation"
author: "Donghyung Lee"
date: "`r Sys.Date()`"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Non-normal errors and Non-constant error variance

Transformations of the response variable Y  can improve the fit and correct violations of **model assumptions such as normality and constant error variance.** 

* **Normality:**
  + Thanks to the Central Limit Theorem, we can actually get away with some mild violation of the normality assumption when ths sample size is large. 
  + If your sample is small, the normality assumption is pretty important.
		
* **Constant Variance:**
  + For test and CI or PI results to be correct, we must assume that the variance of the errors is constant.
  + It is usually a more crucial assumption than normality in validating model inferences.
	

## Box-Cox Method: Transformations of the response variable Y

* The **Box-Cox transformation method** is a popular way to determine a normalizing data transformation for Y. 
		
* It is designed for strictly positive responses $(Y > 0)$ and determines the transformation to find the best fit to the data.  

* In the Box-Cox method, instead of fitting the original Y as the response, we determine a transformed version $t_{\lambda}(Y)$, where

$$
\begin{aligned}
t_{\lambda}(Y)=\left \{\begin{array}{ll}
\frac{Y^{\lambda}-1}{\lambda} & \mbox{for  } \lambda \neq 0\\
ln Y& \mbox{for  } \lambda =0\\
\end{array} \right.
\end{aligned}
$$

* To get the best $\lambda$ estimate, we calculate the log-likelihood of the data, and find $\lambda$ that makes the log-likelihood maximum.

* After the **"power" $\lambda$** is estiamted. We fit the linear model using the transformed response
$$
t_{\lambda}(Y)=\beta_0+\beta_1X_1+\cdots+\beta_kX_k+\epsilon.
$$
		
* For $\lambda=0$
$$
t_{\lambda}(Y)=ln Y=\beta_0+\beta_1X_1+\cdots+\beta_kX_k+\epsilon.
$$

* For $\lambda=-1$
$$
t_{\lambda}(Y)=Y^{-1}=\frac{1}{Y}=\beta_0+\beta_1X_1+\cdots+\beta_kX_k+\epsilon.
$$


## The boxcox() function in the MASS package

* The boxcox function in the MASS package will produce a plot of the log-likelihood against the transformation parameter $\lambda$. 
* Larger log-likelihood is better. 
* Which $\lambda$ corresponds to the largest log-likelihood?

```{r}
library(MASS)
#Cars93 data MPG.highway(Y), Weight(X)
#1. Diagnostic of predictor variables
attach(Cars93)
fit=lm(MPG.highway~Weight)
bc=boxcox(fit)#in library(MASS)
#find lambda
bc$x[which.max(bc$y)]
```
* The $\lambda$ that corresponding to the largest log-likelihood is -0.7070707.
* The optimal choice of power transformation is near $\lambda$ = -0.71. 
* The 95\% confidence interval for $\lambda$ can also be viewed on the graph. 
* For easy interpration, we typically use **one of -1($1/Y$), 0($ln Y$), 0.5 ($\sqrt{Y}$) and 2 ($Y^2$) nearest to the optimum**. So in this problem, we would choose \pause$\lambda =-1$. In other words, instead of fitting $Y$, we use $Y^{-1}=1/Y$.
		

## Addressing Violations of the Normality Assumption
```{r}
fit2=lm(1/MPG.highway~Weight)
plot(fit2,which=2)
par(mfrow=c(1,2))
plot(fit,which=2)
plot(fit2,which=2)

shapiro.test(fit$residuals) #before
shapiro.test(fit2$residuals) #after 
```
Both the graphical check above and the Shapiro-Wilk test shows the violation of the normality assumption before has been solved. Based on the test result, since the p-value is 0.2707, we can conclude the residuals follow a normal distribution.



## Addressing Violations of the Constant Variance Assumption}
```{r}
library(zoo)
library(lmtest)

par(mfrow=c(1,2))
plot(fit, which=3)
plot(fit2,which=3) 

bptest(MPG.highway~Weight)
bptest(1/MPG.highway~Weight)
```
Doing the diagnostic check for constant variance, the graphical check shows the non-constant variance situation has been modified. The BP test also shows the residuals have constanct variance (p-value 0.5016). Sometimes problems of non-normality and non-constant variance go hand-in-hand, so treating one problem frequently cures the other. However, this is not always the case.

## Practical Suggestions

* If both  constant variance assumption and normality assumption are violated, then try a power transformation on $Y$ by Box-Cox method.
* If  the normality assumption is met, but constant  variance assumption is not met, use $\sqrt{Y}$ or $log Y$.
* The log transformation (more frequently used) would be more appropriate for more severe cases. 
* If 1 and 2 do not work, we need more advanced technique such as Weighted least squares (WLS) regression (Chap 11).


## Some notes
$$
\frac{1}{Y}=\beta_0+\beta_1X+\epsilon,
$$
where $Y$ is highway mpg and $X$ is car weight.

* Regression coefficients will need to be interpreted with respect to the **transformed scale.**

* $\beta_0$ is the true mean of the reciprocal of highway mpg at $X=0$ (meaningless in this problem context)

* $\beta_1$ is the change in the true mean of the reciprocal of highway mpg as a car weight increases by 1 pound.

* There is no straightforward way of "untransforming" them to values that can interpreted in the original scale.

* Even if you transform the response, you can **express model predictions back in the original scale.**

* **This is simply a matter of "untransforming" by using the inverse function of the original transformation.**		

<br />


For example, we want to find a 95\% PI for a car weighing 2000 lb.
```{r}
pred=predict(fit2,newdata=data.frame(Weight=2000),int="prediction",level=0.95)
pred
1/pred
```
* We are 95\% confident that the true highway mpg for an unobserved individual car weighing 2000 lb is somewhere between 30.03 to 50.66 miles per gallon.  

* The lower and upper prediction limits in R are switched because of the reciprocal transformation we used.

## Boxcox Transformation When Response Not Positive
* If the reponses are not strictly postive, then add a constant number to reponses first and the use the Box-Cox method. 
$$
(Y+\lambda_2)^{\lambda_1}
$$

$$	
    \begin{eqnarray*}
			t_{\lambda_1,\lambda_2}(Y)=\left \{\begin{array}{ll}
				\frac{(Y+\lambda_2)^{\lambda_1}-1}{\lambda_1} & \mbox{for  } \lambda_1 \neq 0\\
				ln (Y+\lambda_2)& \mbox{for  } \lambda_1=0\\
			\end{array} \right.
		\end{eqnarray*}
$$
